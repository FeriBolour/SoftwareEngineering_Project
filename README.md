# Software Engineering Project
## Introduction:
With everything going on in a post-modern COVID world many businesses, churches, and schools have to transfer online. With this happening, us as consumers who interact with these establishments have transitioned online. Many people have had trouble transitioning from hands on work to working from home. It could be as simple as just opening a document, but if someone who has not worked on a device; this could be a great challenge. What if you were able to raise your hand and it opens up Microsoft Word? Essentially some basic hand gestures that allow users who are having a hard time transition electronically or experienced user improve their workflow increasing productivity. Working from home has us constantly using the same applications on our computers, one simple hand gesture that could open an entire suite of applications a user needs. A photographer could hold their fist up and Adobe Lightroom and Photoshop open in a pinch, a music producer holds up a peace sign opens Ableton and Audacity, or even programmers could hold up an L and it opens Visual Studio and WSL terminal. Our ambition is getting our prototype to open applications on our devices by doing simple hand gestures. Python was chosen as it is quick to pick up and it is a great scripting language that is great for machine learning. It is a simple language that the majority of us had experience with. While Python is not the fastest it is a trade-off that we are willing to accept. Going to get live feed from any camera and programmatically detect gestures that the user presents. Using OpenCV and Python we will get the live feed running. Using this feed we will extract the hand gesture. Going to need a lot of data for the model to work accurately, such as multiple images to feed the model. It is crucial that we have a robust data set when it comes to training data having the same gestures with large varieties so the model will be more accurate. Our top 5 features we had come up with in the prototype are extracting hand gestures, building a deep learning model, preprocessing our dataset, training, testing, and evaluating our model and finally assigning different tasks to each hand gesture prediction. 

# Release 1:
**1. 	Extracting Hand Gestures:**  The first main task that we had to figure out for this project is to accurately extract the hand gestures while the live feed camera is running. The approach that we took in order to accomplish this task was an image processing technique called ‘Background Subtraction’. Basically the only requirement is to first take a photo of a scene before starting to show your hand gestures in it. Then we can create a ‘mask’ of that scene and then use it later when you are showing your hand to subtract the background (original scene) from it. 
 
  After Subtracting the scene, we used a Gaussian Blurring (Smoothing) filter to eliminate unnecessary details of the hand. And then we used a simple binary thresholding filter to make the target gesture (our hand) totally white and the background totally black. This will make it easier for the model to generalize between different skin colors and also will reduce the computational complexity of our process since now the model needs to do the inference on a Black and White image rather than a RGB image.
 
# Release 2:
**2.	Building a Deep Learning Model:** Choosing the right architecture for our Convolutional Neural Network is the step that determines how good our model is going to be. We are going to be using Keras and TensorFlow framework and libraries for building and training our model. We are also thinking of using a transfer learning technique and using a popular pre-trained model like VGG-16 with some added layers to adjust it to our purposes, rather than building our own convolutional neural network from scratch. 

**3.	Preprocessing the Dataset:** After finalizing our CNN architecture, then we need to adjust the images in our dataset to the requirements of our input model. For example, our architecture would only accept a certain image size as its input for inference, so we need to resize each image before loading it into the model. Or some images in our dataset could be RGB images and we might need to convert them into BW images. And many other possible modifications. 

**4.	Training, Testing, and Evaluation of our model:** Next step would be randomly splitting our dataset into training and testing sets. Then first, training our model using the training set and using the trained model for prediction on the testing set. Since we have the ground truth (gestures) of our testing set, we can evaluate how good our model is.  Then we can repeat this process while modifying/tuning our training parameters (such as our optimizer, loss function, learning rate, and number of epochs) until we reach optimal and desired results.
